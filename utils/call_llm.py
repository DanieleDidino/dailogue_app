import openai
from utils.prompts import create_dynamic_prompt


def count_token_usage(api_response: openai.types.chat.chat_completion.ChatCompletion) -> tuple:
    """
    Count how many token have been sent to the API.
    This version is specific for OpenAI and Streamlit.

    Args:
        api_response: Object returned by a synchronous openai client instance

    Returns:
        (tuple): the number of tokens used, that is
            - 'completion_tokens': token generated by the model.
            - 'prompt_tokens': tokens sent to the api.
            - 'total_tokens': total count of used tokens.
    """

    completion_tokens = api_response.usage.completion_tokens
    prompt_tokens = api_response.usage.prompt_tokens
    total_tokens = api_response.usage.total_tokens

    return (completion_tokens, prompt_tokens, total_tokens)


def call_api_client(api_client: openai.OpenAI, llm_model: str, system_prompt: str, prompt: str, temperature: float) -> tuple:
    """
    Call the API and get the response.

    Args:
        api_client: Object that manage the call to OpenAI API.
        llm_model (string): Name of the OpenAI model.
        system_prompt (string): A prompt used to inform the model how to modify the user text.
        prompt (string): Prompt including the instructions and the original text provided by the user.
        temperature (float): Temperature for the OpenAI model.

    Returns:
        response (string): response from api (a label, edited text, or some text)
        token_usage (tuple): the number of tokens used
    """

    chat_completion = api_client.chat.completions.create(
        model=llm_model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=temperature
    )

    response = chat_completion.choices[0].message.content

    token_usage = count_token_usage(chat_completion)

    return response, token_usage


def get_responses_from_llm(prompt: str, api_client: openai.OpenAI, llm_model: str, temperature: float) -> tuple[str, str]:
    """
    This function perform the following steps:
        1) Call a function to create a prompt to split and classify the text.
        2) Call the API with the create prompt to get the text classification.
        3) Call a function to create a prompt to use to edit the text into a more functional version.
        4) Call the API with the second prompt to get the modified functional text.

    Args:
        prompt (string): Prompt to pass to the OpenAI model.  
        api_client: Object that manage the call to OpenAI API.
        llm_model (string): Name of the OpenAI model.
        temperature (float): Temperature for the OpenAI model.

    Returns:
        A tuple with the following objects:
            - 'edited_text': Text converted in a more functional version.
            - 'total_token': total count of used tokens.
    """

    # TODO: edit the funtion to call the API with the dynamic few-shot prompt
    transformed_text, token_usage = call_api_client(
        api_client= api_client,
        llm_model=llm_model,
        system_prompt="",
        prompt=prompt,
        temperature=temperature)
    
    # Get total token usage
    _, _, total_token = token_usage

    return transformed_text, total_token
